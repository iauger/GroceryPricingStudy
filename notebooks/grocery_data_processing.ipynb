{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the Data Processing Notebook\n",
    "\n",
    "## Overview\n",
    "This notebook documents the **data processing pipeline** for the raw data we've fetched from the Kroger API and Census Bureau. The primary goal of this section is to **clean and structure raw data** to create a well-integrated dataset that can be used for further analysis and visualization.The goal at the end of this notebook will be to have structured a dataset that is organized around ZIP Codes, and presents Users with Location, Product and Population data representative of Kroger stores within that ZIP Code. Once we achieve,  we can move forward with Analysis and Visualization. \n",
    "\n",
    "## 📊 Data Sources\n",
    "The following datasets have been extracted and processed:\n",
    "\n",
    "### 1. **Kroger API (Product Data)**\n",
    "- Retrieves grocery product details, including prices, brand, category, and promotional discounts.\n",
    "- Data is fetched dynamically for selected ZIP codes.\n",
    "\n",
    "### 2. **Kroger API (Store Locations)**\n",
    "- Provides the geolocation of Kroger store locations.\n",
    "- Used to map grocery pricing data to specific ZIP codes.\n",
    "\n",
    "### 3. **U.S. Census Data**\n",
    "- Includes demographic and economic indicators such as:\n",
    "  - Median household income\n",
    "  - Poverty rate\n",
    "  - SNAP participation rate\n",
    "  - Racial and Ethnic identifiers\n",
    "  - Educational attainment levels\n",
    "\n",
    "### 4. **ZIP Code Geospatial Data**\n",
    "- Used to create accurate geographic boundaries for ZIP codes.\n",
    "- Enables mapping and spatial analysis of grocery pricing.\n",
    "\n",
    "## **Goal of the Data Processing Section**\n",
    "The objective of this notebook is to:\n",
    "1. **Clean** and standardize the datasets (handling missing values, filtering, formatting).\n",
    "2. **Merge** data sources to form a unified dataset that connects pricing, location, and demographics.\n",
    "3. **Optimize** the dataset for visualization and statistical analysis.\n",
    "\n",
    "This processed dataset will serve as the foundation for price trend analysis, correlation studies, and geospatial visualizations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Data Cleaning\n",
    "\n",
    "### Overview\n",
    "This module processes and normalizes **U.S. Census data** to enable meaningful comparisons across ZIP codes. By converting raw counts into **percentage-based metrics**, it ensures consistency when analyzing **socioeconomic factors and grocery pricing**.\n",
    "\n",
    "### Data Processed\n",
    "#### Raw Census Data\n",
    "- ZIP-level statistics on **income, poverty, education, and SNAP participation**.\n",
    "- Total population counts and racial/ethnic distribution.\n",
    "\n",
    "#### Processed Census Data\n",
    "- Converts raw counts into **percentage-based indicators**.\n",
    "- Removes unnecessary columns and fills missing values.\n",
    "- Formats ZIP codes as strings for dataset merging.\n",
    "\n",
    "### Purpose\n",
    "The cleaned census data allows us to:\n",
    "- **Analyze socioeconomic trends** in grocery pricing.\n",
    "- **Integrate demographic data** with store locations and product prices.\n",
    "- **Support correlation and geospatial analysis** on food accessibility.\n",
    "\n",
    "This dataset serves as a foundation for **pricing disparities and affordability studies** across different communities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/data_processing/`\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"../data\")  # Navigate to `src/data/`\n",
    "\n",
    "# Define file paths\n",
    "CENSUS_FILE = os.path.join(DATA_DIR, \"cleaned_census_data.csv\")\n",
    "PROCESSED_CENSUS_FILE = os.path.join(DATA_DIR, \"processed_census_data.csv\")\n",
    "\n",
    "def process_census_data():\n",
    "    \"\"\"Cleans, normalizes, and integrates ZIP Code boundary data with census data.\"\"\"\n",
    "    if not os.path.exists(CENSUS_FILE):\n",
    "        print(\"Census data file not found!\")\n",
    "        return None\n",
    "    \n",
    "    # Load census data\n",
    "    census_df = pd.read_csv(CENSUS_FILE, dtype={\"ZIP Code\": str})\n",
    "    \n",
    "    # Normalize percentage-based fields\n",
    "    census_df[\"Poverty Rate (%)\"] = census_df[\"Poverty Count\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"SNAP Participation (%)\"] = census_df[\"SNAP Households\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"White Population (%)\"] = census_df[\"White Population\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"Black Population (%)\"] = census_df[\"Black Population\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"American Indian Population (%)\"] = census_df[\"American Indian Population\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"Asian Population (%)\"] = census_df[\"Asian Population\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"Other Race Population (%)\"] = (census_df[\"Other Race Population\"] + census_df[\"Pacific Islander Population\"]) / census_df[\"Total Population\"]\n",
    "    census_df[\"Two or More Races (%)\"] = census_df[\"Two or More Races Population\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"High School Graduate (%)\"] = census_df[\"High School Graduate\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"Bachelor's Degree (%)\"] = census_df[\"Bachelor's Degree\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"Master's Degree (%)\"] = census_df[\"Master's Degree\"] / census_df[\"Total Population\"]\n",
    "    census_df[\"Doctorate Degree (%)\"] = census_df[\"Doctorate Degree\"] / census_df[\"Total Population\"]\n",
    "    \n",
    "    # Drop original count-based columns\n",
    "    drop_cols = [\n",
    "        \"Poverty Count\", \"SNAP Households\",\n",
    "        \"White Population\", \"Black Population\", \"American Indian Population\", \"Pacific Islander Population\",\n",
    "        \"Asian Population\", \"Other Race Population\", \"Two or More Races Population\",\n",
    "        \"High School Graduate\", \"Bachelor's Degree\", \"Master's Degree\", \"Doctorate Degree\"\n",
    "    ]\n",
    "    census_df.drop(columns=drop_cols, inplace=True)\n",
    "    \n",
    "    # Fill NaN values (from divide by zero) with 0\n",
    "    census_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Ensure ZIP is a string for merging\n",
    "    census_df[\"ZIP Code\"] = census_df[\"ZIP Code\"].astype(str)\n",
    "    \n",
    "    # Ensure all numeric columns are correctly formatted\n",
    "    census_df.set_index(\"ZIP Code\", inplace=True)\n",
    "    census_df = census_df.astype(float)  # Convert everything else to float\n",
    "    census_df.reset_index(inplace=True)\n",
    "    \n",
    "    # Save processed census data\n",
    "    census_df.to_csv(PROCESSED_CENSUS_FILE, index=False)\n",
    "    print(f\"Processed census data saved to {PROCESSED_CENSUS_FILE}\")\n",
    "    return census_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_census_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Data Cleaning\n",
    "\n",
    "### Overview\n",
    "This module **cleans and standardizes store location data**, ensuring that each store is accurately geocoded and linked to the correct ZIP code. Missing latitude, longitude, and ZIP codes are resolved using the **Google Maps API** to enable accurate **spatial analysis and data integration**.\n",
    "\n",
    "### Data Processed\n",
    "#### Raw Location Data\n",
    "- Contains **store details** such as `Location ID`, `Address`, `City`, `State`, and `ZIP Code`.\n",
    "- Some entries may lack **geospatial coordinates** or have incorrect ZIP codes.\n",
    "- ZIP Code inconsistencies are possibly a biproduct of the Location API, likely storing the search ZIP Code instead of the Address ZIP Code\n",
    "\n",
    "#### Cleaned Location Data\n",
    "- **Geocodes missing store locations** using external API calls.\n",
    "- **Adds or corrects ZIP codes** based on retrieved geolocation data.\n",
    "- **Ensures consistency** in data formats for merging with product and census data.\n",
    "\n",
    "### Purpose\n",
    "By refining store location accuracy, this module enables:\n",
    "- **Accurate ZIP-code-based pricing analysis.**\n",
    "- **Reliable geospatial mapping of grocery store distribution.**\n",
    "- **Seamless integration with product pricing and demographic datasets.**\n",
    "\n",
    "The cleaned store location dataset ensures **consistency and accuracy** in downstream analyses. \n",
    "ZIP Code correction  is essential here because we will use this as the primary key to group and join with Census data during the merge phase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import googlemaps\n",
    "import time\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/data_processing/`\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"../data\")  # Navigate to `src/data/`\n",
    "\n",
    "# Define file paths\n",
    "LOCATIONS_FILE = os.path.join(DATA_DIR, \"kroger_locations.csv\")\n",
    "CLEANED_LOCATIONS_FILE = os.path.join(DATA_DIR, \"cleaned_location_data.csv\")\n",
    "\n",
    "# # Load Google Maps API Key from environment\n",
    "# GMAPS_API_KEY = os.getenv(\"GOOGLE_MAPS_API_KEY\")\n",
    "# if not GMAPS_API_KEY:\n",
    "#     raise ValueError(\"Google Maps API Key is missing! Set it as an environment variable.\")\n",
    "\n",
    "GOOGLE_API_KEY = \"AIzaSyDRBpE236p2NPURBmuWEIZ1FlhYjb04dxk\" \n",
    "gmaps = googlemaps.Client(key=GOOGLE_API_KEY)\n",
    "\n",
    "# Define data types\n",
    "dtype_dict = {\n",
    "    \"Location ID\": str,\n",
    "    \"ZIP Code\": str,\n",
    "    \"Division Number\": str,\n",
    "    \"Store Number\": str,\n",
    "    \"Latitude\": float,\n",
    "    \"Longitude\": float\n",
    "}\n",
    "\n",
    "def fetch_geocode(address):\n",
    "    \"\"\"Fetches lat/lon and ZIP code for an address with retry logic.\"\"\"\n",
    "    attempts = 3  # Retry up to 3 times\n",
    "    for attempt in range(attempts):\n",
    "        try:\n",
    "            result = gmaps.geocode(address)\n",
    "            if result:\n",
    "                location = result[0]['geometry']['location']\n",
    "                zip_code = None\n",
    "                for component in result[0]['address_components']:\n",
    "                    if 'postal_code' in component['types']:\n",
    "                        zip_code = component['long_name']\n",
    "                        break\n",
    "                return location['lat'], location['lng'], zip_code\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1}: Geocoding failed for {address}: {e}\")\n",
    "            time.sleep(1) \n",
    "    return None, None, None\n",
    "\n",
    "def geocode_store_locations():\n",
    "    \"\"\"Geocodes missing locations and updates ZIP codes in the cleaned dataset.\"\"\"\n",
    "    if not os.path.exists(LOCATIONS_FILE):\n",
    "        print(\"Location data file not found!\")\n",
    "        return None\n",
    "\n",
    "    locations_df = pd.read_csv(LOCATIONS_FILE, dtype=dtype_dict)\n",
    "    locations_df[\"Full Address\"] = locations_df.apply(lambda row: f\"{row['Address']}, {row['City']}, {row['State']}, USA\", axis=1)\n",
    "    \n",
    "    if os.path.exists(CLEANED_LOCATIONS_FILE):\n",
    "        cleaned_df = pd.read_csv(CLEANED_LOCATIONS_FILE, dtype=dtype_dict)\n",
    "        processed_ids = set(cleaned_df[\"Location ID\"].astype(str))\n",
    "        new_locations = locations_df[~locations_df[\"Location ID\"].astype(str).isin(processed_ids)]\n",
    "    else:\n",
    "        cleaned_df = locations_df.copy()\n",
    "        cleaned_df[[\"Latitude\", \"Longitude\"]] = None\n",
    "    \n",
    "    if new_locations.empty:\n",
    "        print(\"All locations already processed. Returning existing cleaned file.\")\n",
    "        return cleaned_df\n",
    "    \n",
    "    for index, row in new_locations.iterrows():\n",
    "        address = row[\"Full Address\"]\n",
    "        lat, lon, zip_code = fetch_geocode(address)\n",
    "        new_locations.at[index, \"Latitude\"] = lat\n",
    "        new_locations.at[index, \"Longitude\"] = lon\n",
    "        new_locations.at[index, \"ZIP Code\"] = zip_code  # Replacing existing ZIP Code values\n",
    "        time.sleep(1)  # Prevent rate limiting\n",
    "    \n",
    "    updated_df = pd.concat([cleaned_df, new_locations], ignore_index=True)\n",
    "    updated_df.to_csv(CLEANED_LOCATIONS_FILE, index=False)\n",
    "    print(f\"Cleaned location data saved to {CLEANED_LOCATIONS_FILE}\")\n",
    "    return updated_df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    geocode_store_locations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🛒 Product Data Cleaning\n",
    "\n",
    "### Overview\n",
    "This module processes and cleans product data retrieved from the **Kroger API**. It ensures consistency across product attributes, removes duplicates, and filters out inactive listings. The cleaned dataset enables accurate **price analysis, category comparisons, and integration with location and demographic data**.\n",
    "\n",
    "### Data Processed\n",
    "#### Raw Product Data\n",
    "- Contains **product descriptions, prices, stock levels, and store locations**.\n",
    "- May include **duplicates, missing values, or inconsistencies**.\n",
    "\n",
    "#### Cleaned Product Data\n",
    "- **Categorizes products** (`Egg`, `Bread`, `Other`) based on descriptions.\n",
    "- **Extracts quantity and unit of measure** from package sizes.\n",
    "- **Normalizes price fields** and computes per-unit costs.\n",
    "- **Removes invalid or inactive records**.\n",
    "- **Eliminates duplicate product-location entries** \n",
    "\n",
    "### Purpose\n",
    "Cleaning the product data ensures:\n",
    "- **Accurate pricing comparisons** across ZIP codes.\n",
    "- **Consistency in product classification** for category-level analysis.\n",
    "- **Better integration** with store locations and census demographics.\n",
    "\n",
    "This data is by far the noisest within the study, so this amount of cleaning was critical to ensuring better aggregation and integration downstream. I think there's  still a huge opportunity to improve the quality and comprehensiveness of the product data, but I think the work done here will enable the improvement of data quality in the long run.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/data_processing/`\n",
    "DATA_DIR = os.path.join(BASE_DIR, \"../data\")  # Navigate to `src/data/`\n",
    "\n",
    "# Define file paths\n",
    "PRODUCTS_FILE = os.path.join(DATA_DIR, \"kroger_product_data.csv\")\n",
    "CLEANED_PRODUCTS_FILE = os.path.join(DATA_DIR, \"cleaned_product_data.csv\")\n",
    "\n",
    "# Define data types\n",
    "dtype_dict = {\n",
    "    \"Product ID\": str,\n",
    "    \"UPC\": str,\n",
    "    \"Location ID\": str,\n",
    "    \"ZIP Code\": str\n",
    "}\n",
    "\n",
    "def clean_product_data():\n",
    "    \"\"\"Loads, cleans, and standardizes product data.\"\"\"\n",
    "    if not os.path.exists(PRODUCTS_FILE):\n",
    "        print(\"Product data file not found!\")\n",
    "        return None\n",
    "\n",
    "    df = pd.read_csv(PRODUCTS_FILE, dtype=dtype_dict)\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"Warning: Product data is empty! Skipping processing.\")\n",
    "        return None\n",
    "        \n",
    "    # Function to pad Location IDs to 8 digits\n",
    "    def pad_location_id(location_id):\n",
    "        return location_id.zfill(8)  # Ensures length is always 8 by adding leading zeros\n",
    "\n",
    "    # Apply function to correct the Location ID length\n",
    "    df[\"Location ID\"] = df[\"Location ID\"].apply(pad_location_id)\n",
    "\n",
    "    # Standardize category names \n",
    "    # This method is potentially flawed and I did incorporate a regex match at one point\n",
    "    # While not  ideal, this method showed the best results\n",
    "    def classify_product(description):\n",
    "        desc = description.lower()\n",
    "        if \"egg\" in desc:\n",
    "            return \"Egg\"\n",
    "        elif \"bread\" in desc:\n",
    "            return \"Bread\"\n",
    "        else:\n",
    "            return \"Other\"\n",
    "\n",
    "    df[\"Product Category\"] = df[\"Description\"].apply(classify_product)\n",
    "    \n",
    "    # Extract Quantity and UOM from Size element\n",
    "    def process_size(df):\n",
    "        def extract_quantity(size):\n",
    "            match = re.search(r'([\\d\\.]+)', str(size))\n",
    "            return float(match.group(1)) if match else -1 \n",
    "        \n",
    "        def extract_uom(size):\n",
    "            match = re.search(r'[a-zA-Z]+.*$', str(size))\n",
    "            return match.group(0).strip() if match else \"unit\"\n",
    "\n",
    "        # Apply extraction functions\n",
    "        df[\"Quantity\"] = df[\"Size\"].apply(extract_quantity)\n",
    "        df[\"UOM\"] = df[\"Size\"].apply(extract_uom)\n",
    "\n",
    "        return df\n",
    "\n",
    "    # Apply function to product dataset\n",
    "    df = process_size(df)\n",
    "    \n",
    "    # Normalize prices, ensuring non-numeric values are handled\n",
    "    df[\"Regular Price\"] = pd.to_numeric(df[\"Regular Price\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Promo Price\"] = pd.to_numeric(df[\"Promo Price\"], errors=\"coerce\").fillna(0)\n",
    "    df[\"Price per Unit\"] = df[\"Regular Price\"] / df[\"Quantity\"]\n",
    "    df[\"Promo Price per Unit\"] = df[\"Promo Price\"] / df[\"Quantity\"]\n",
    "\n",
    "    # There is no explicit Inactive tag in the product data, \n",
    "    # but this mix of variables makes the product unactionable and therefore I removed them from the dataset\n",
    "    inactive_items_mask = (df[\"Stock Level\"].str.lower() == \"unknown\") & \\\n",
    "                          (df[\"Regular Price\"] == 0) & \\\n",
    "                          (df[\"Promo Price\"] == 0)\n",
    "\n",
    "    df = df.loc[~inactive_items_mask].copy()\n",
    "        \n",
    "    df = df.drop_duplicates(subset=[\"Product ID\", \"Location ID\", \"Date Retrieved\"])\n",
    "\n",
    "    # Save cleaned data\n",
    "    df.to_csv(CLEANED_PRODUCTS_FILE, index=False)\n",
    "    print(f\"Cleaned product data saved to {CLEANED_PRODUCTS_FILE}\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    clean_product_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📍 ZIP Code Data Merging\n",
    "\n",
    "### Overview\n",
    "This module integrates multiple datasets—including **product data, store locations, census data, and ZIP code boundaries**—to generate a comprehensive dataset for analysis. We've reviewed the acquisition and construction process for these individual datasets above and in other notebooks,  the work done prior to this module will enable the centralization of data around individual ZIP Codes. \n",
    "\n",
    "### Data Processed\n",
    "#### Input Datasets\n",
    "- **Cleaned Product Data** (`cleaned_product_data.csv`) – Pricing and availability of grocery items.\n",
    "- **Cleaned Store Locations** (`cleaned_location_data.csv`) – Geocoded locations of Kroger stores.\n",
    "- **Processed Census Data** (`processed_census_data.csv`) – Socioeconomic indicators for ZIP codes.\n",
    "- **ZIP Code Boundaries** (`tl_2020_us_zcta520.shp`) – Geospatial boundaries for mapping.\n",
    "\n",
    "#### Merged Dataset (`final_dataset.csv`)\n",
    "- **Combines product prices, store locations, and census data by ZIP code**.\n",
    "- **Aggregates store count and chain distribution per ZIP**.\n",
    "- **Computes average product prices, promotional activity, and price volatility**.\n",
    "- **Generates keyword frequency data for product descriptions**.\n",
    "\n",
    "### Purpose\n",
    "This module prepares the dataset for:\n",
    "- **Geospatial analysis of grocery pricing trends**.\n",
    "- **Correlation studies between food prices and socioeconomic factors**.\n",
    "- **Visualizations of grocery access at the ZIP code level**.\n",
    "\n",
    "The final dataset serves as the foundation for statistical modeling and mapping.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import Counter \n",
    "import geopandas as gpd\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/data_processing/`\n",
    "DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"data\"))\n",
    "\n",
    "# Define file paths\n",
    "SHAPEFILE = os.path.join(DATA_DIR, \"2020_ZCTA\", \"tl_2020_us_zcta520.shp\")\n",
    "CLEANED_PRODUCTS_FILE = os.path.join(DATA_DIR, \"cleaned_product_data.csv\")\n",
    "CLEANED_LOCATIONS_FILE = os.path.join(DATA_DIR, \"cleaned_location_data.csv\")\n",
    "PROCESSED_CENSUS_FILE = os.path.join(DATA_DIR, \"processed_census_data.csv\")\n",
    "FINAL_DATASET = os.path.join(DATA_DIR, \"final_dataset.csv\")\n",
    "\n",
    "# Define data types\n",
    "dtype_dict = {\n",
    "    \"Location ID\": str,\n",
    "    \"ZIP Code\": str,\n",
    "    \"Division Number\": str,\n",
    "    \"Store Number\": str,\n",
    "    \"Latitude\": float,\n",
    "    \"Longitude\": float,\n",
    "    \"Product ID\": str,\n",
    "    \"UPC\": str\n",
    "}\n",
    "\n",
    "# Load census data and ensure data types are properly set\n",
    "if not os.path.exists(PROCESSED_CENSUS_FILE):\n",
    "    print(\"Census data file not found!\")\n",
    "else:\n",
    "    census_df = pd.read_csv(PROCESSED_CENSUS_FILE, dtype=dtype_dict)\n",
    "\n",
    "# Ensure ZIP is a string for merging\n",
    "census_df[\"ZIP Code\"] = census_df[\"ZIP Code\"].astype(str)\n",
    "\n",
    "# Ensure all numeric columns are correctly formatted\n",
    "census_df.set_index(\"ZIP Code\", inplace=True)\n",
    "census_df = census_df.astype(float)  # Convert everything else to float\n",
    "census_df.reset_index(inplace=True)\n",
    "\n",
    "# Load ZIP boundaries shapefile\n",
    "if os.path.exists(SHAPEFILE):\n",
    "    zip_boundaries = gpd.read_file(SHAPEFILE)\n",
    "    zip_boundaries[\"ZIP Code\"] = zip_boundaries[\"ZCTA5CE20\"].astype(str)\n",
    "    \n",
    "    # Merge census data with shapefile data\n",
    "    # Shapefile is merged to Census initially since Census data is already organized by ZIP Code\n",
    "    census_df = census_df.merge(zip_boundaries, on=\"ZIP Code\", how=\"left\").copy()\n",
    "    census_df = gpd.GeoDataFrame(census_df, geometry=\"geometry\")\n",
    "else:\n",
    "    print(\"Shapefile not found, skipping ZIP boundary integration.\")\n",
    "    \n",
    "if not os.path.exists(CLEANED_LOCATIONS_FILE):\n",
    "    print(\"Location data file not found!\")\n",
    "else:\n",
    "    location_df = pd.read_csv(CLEANED_LOCATIONS_FILE, dtype=dtype_dict)\n",
    "\n",
    "# Store the frequency and name of chains in each ZIP Code\n",
    "# Not using this data in the analysis but could be useful or at  least interesting in the future\n",
    "def count_store_chains(chain_list):\n",
    "    return dict(Counter(chain_list))\n",
    "\n",
    "zip_location_summary = location_df.groupby(\"ZIP Code\").agg(\n",
    "    Store_Count=(\"Location ID\", \"count\"),  # Number of stores per ZIP\n",
    "    Store_Chain_Distribution=(\"Chain Name\", lambda x: count_store_chains(x)),\n",
    "    Avg_Latitude=(\"Latitude\", \"mean\"),  # Average latitude\n",
    "    Avg_Longitude=(\"Longitude\", \"mean\"),  # Average longitude\n",
    "    ).reset_index()\n",
    "\n",
    "# Merge enhanced census data with aggregated location data\n",
    "zip_location_summary = zip_location_summary.merge(\n",
    "    census_df, \n",
    "    on=\"ZIP Code\", \n",
    "    how=\"left\").copy()\n",
    "\n",
    "if not os.path.exists(CLEANED_PRODUCTS_FILE):\n",
    "    print(\"Product data file not found!\")\n",
    "else:\n",
    "    product_df = pd.read_csv(CLEANED_PRODUCTS_FILE, dtype=dtype_dict)\n",
    "\n",
    "# Ensure date is in datetime format\n",
    "product_df.loc[:, \"Date Retrieved\"] = pd.to_datetime(product_df[\"Date Retrieved\"])\n",
    "\n",
    "# Group by Product ID & Location ID\n",
    "# Calculate per product summary statistics\n",
    "# Will be more interesting data over a longer acquisition period depending  on pricing fluctuations. \n",
    "product_grouped_df = product_df.groupby([\"Product ID\", \"Location ID\", \"Quantity\", \"UOM\", \"Brand\", \"Product Category\", \"Description\"]).agg(\n",
    "    Most_Recent_Date=(\"Date Retrieved\", \"max\"),\n",
    "    Most_Recent_Price=(\"Price per Unit\", lambda x: x.iloc[-1] if not x.empty else None),  # Handle empty groups safely\n",
    "    Avg_Price=(\"Price per Unit\", \"mean\"),\n",
    "    Promo_Price_Avg=(\"Promo Price per Unit\", lambda x: x[x > 0].mean()),  # Avg promo price (ignoring zeros)\n",
    "    Price_Volatility=(\"Price per Unit\", \"std\"),  # Std deviation of price\n",
    "    Promo_Observations=(\"Promo Price\", lambda x: (x > 0).sum()),  # Count promo instances\n",
    "    Total_Observations=(\"Date Retrieved\", \"count\")  # Count total entries\n",
    ").reset_index()\n",
    "\n",
    "# Fill missing values (NaNs) with 0 where applicable\n",
    "product_grouped_df.fillna({\"Price_Volatility\": 0, \"Promo_Price_Avg\": 0, \"Most_Recent_Price\": 0}, inplace=True)\n",
    "\n",
    "# Calculate % of runs containing promo\n",
    "product_grouped_df[\"Promo_Frequency\"] = (\n",
    "    product_grouped_df[\"Promo_Observations\"] / product_grouped_df[\"Total_Observations\"]\n",
    ").fillna(0).round(2)\n",
    "\n",
    "# Tokenize words in Description\n",
    "product_grouped_df[\"Description List\"] = product_grouped_df[\"Description\"].str.lower().str.split()\n",
    "\n",
    "# Aggregate price & stock availability per category per store\n",
    "# Calculate summary statistics per category (egg, bread, other)\n",
    "store_product_summary = product_grouped_df.groupby([\"Location ID\", \"Product Category\", \"UOM\"]).agg(\n",
    "    Product_Count=(\"Product ID\", \"count\"),\n",
    "    Stock_Observations=(\"Total_Observations\", \"max\"),\n",
    "    Avg_Price=(\"Avg_Price\", \"mean\"),\n",
    "    Min_Price=(\"Avg_Price\", \"min\"),\n",
    "    Max_Price=(\"Avg_Price\", \"max\"),\n",
    "    Median_Price=(\"Avg_Price\", \"median\"),\n",
    "    Price_Volatility=(\"Price_Volatility\", \"mean\"),\n",
    "    Promo_Frequency=(\"Promo_Frequency\", \"mean\")    \n",
    ").reset_index()\n",
    "\n",
    "def clean_keyword(word):\n",
    "    word = re.sub(r\"[^a-zA-Z]\", \"\", word).lower()  # Remove non-alphabet characters & lowercase\n",
    "    if word in [\"egg\", \"eggs\", \"bread\"]:  # Exclude these words\n",
    "        return None\n",
    "    return word\n",
    "\n",
    "# Create a list of all words found  in all descriptions per product category\n",
    "# Critical to understand the types of products being offered per store/region and to whom\n",
    "def keyword_list(description_lists):    \n",
    "    category_list = []\n",
    "    for word_list in description_lists:\n",
    "        cleaned_words = [clean_keyword(word) for word in word_list if clean_keyword(word)]\n",
    "        category_list.extend(cleaned_words)  # Use extend() instead of appending lists\n",
    "        \n",
    "    return category_list if category_list else [\"other\"]\n",
    "\n",
    "\n",
    "# Compute keyword frequency for each location-category pair\n",
    "keyword_summary = product_grouped_df.groupby([\"Location ID\", \"Product Category\"])[\"Description List\"].apply(list).apply(keyword_list).reset_index()\n",
    "keyword_summary.rename(columns={\"Description List\": \"Keyword_Lists\"}, inplace=True)\n",
    "\n",
    "store_product_summary = store_product_summary.merge(keyword_summary, on=[\"Location ID\", \"Product Category\"], how=\"left\")\n",
    "\n",
    "# Extract relevant columns: Location ID and ZIP Code\n",
    "loc_zip_df = location_df[[\"Location ID\", \"ZIP Code\"]].copy()\n",
    "\n",
    "# Merge ZIP Code information into the product dataset\n",
    "store_product_summary = store_product_summary.merge(\n",
    "    location_df[[\"Location ID\", \"ZIP Code\"]], on=\"Location ID\", how=\"left\")\n",
    "\n",
    "# Group location product data by ZIP Code\n",
    "zip_product_summary = store_product_summary.groupby([\"ZIP Code\", \"Product Category\"]).agg(\n",
    "    Avg_Price=(\"Avg_Price\", \"mean\"),\n",
    "    Min_Price=(\"Min_Price\", \"min\"),\n",
    "    Max_Price=(\"Max_Price\", \"max\"),\n",
    "    Median_Price=(\"Median_Price\", \"mean\"),\n",
    "    Price_Volatility=(\"Price_Volatility\", \"mean\"),\n",
    "    Promo_Frequency=(\"Promo_Frequency\", \"mean\"),\n",
    "    Avg_Product_Count=(\"Product_Count\", \"mean\") \n",
    ").reset_index()\n",
    "\n",
    "# Process all  keyword lists into a dictionary containing  words and frequencies\n",
    "# Initialize Empty List for Aggregation\n",
    "zip_keyword_freq_list = []\n",
    "\n",
    "# Iterate Over Each Row in zip_product_summary\n",
    "for _, row in zip_product_summary.iterrows():\n",
    "    zip_code = row[\"ZIP Code\"]\n",
    "    category = row[\"Product Category\"]\n",
    "    \n",
    "    # Filter product_zip_df for matching ZIP & Category\n",
    "    matching_rows = store_product_summary[\n",
    "        (store_product_summary[\"ZIP Code\"] == zip_code) & \n",
    "        (store_product_summary[\"Product Category\"] == category)\n",
    "    ]\n",
    "    \n",
    "    # Flatten all keyword lists from matching rows\n",
    "    all_keywords = []\n",
    "    for keyword_list in matching_rows[\"Keyword_Lists\"]:\n",
    "        all_keywords.extend(keyword_list)  # Merge into single list\n",
    "    \n",
    "    # Compute word frequency\n",
    "    keyword_frequency = dict(Counter(all_keywords))  # Convert to dictionary\n",
    "    \n",
    "    # Append to list\n",
    "    zip_keyword_freq_list.append({\n",
    "        \"ZIP Code\": zip_code,\n",
    "        \"Product Category\": category,\n",
    "        \"ZIP_Keyword_Frequency\": keyword_frequency  # Store dictionary\n",
    "    })\n",
    "\n",
    "# Convert into DataFrame\n",
    "zip_keyword_summary = pd.DataFrame(zip_keyword_freq_list)\n",
    "\n",
    "zip_product_complete = zip_product_summary.merge(\n",
    "    zip_keyword_summary, \n",
    "    on=[\"ZIP Code\", \"Product Category\"], \n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Fill missing keyword frequencies with empty dictionaries\n",
    "zip_product_complete[\"ZIP_Keyword_Frequency\"] = zip_product_complete[\"ZIP_Keyword_Frequency\"].apply(lambda x: x if isinstance(x, dict) else {})\n",
    "\n",
    "# Merge finalized Product data with merged Location and Census data\n",
    "zip_product_location_complete = zip_location_summary.merge(\n",
    "    zip_product_complete,\n",
    "    on=\"ZIP Code\",\n",
    "    how=\"left\")\n",
    "\n",
    "# Remove rows with any null values\n",
    "zip_product_location_filtered = zip_product_location_complete.dropna()\n",
    "\n",
    "# Convert to GeoDataFrame for mapping functionality\n",
    "zip_product_location_final = gpd.GeoDataFrame(zip_product_location_filtered)\n",
    "\n",
    "# # Save final dataset\n",
    "zip_product_location_final.to_csv(FINAL_DATASET, index=False)\n",
    "print(f\"Final dataset saved to {FINAL_DATASET}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 Data Processing Execution Script\n",
    "\n",
    "### Overview\n",
    "This script **orchestrates the execution of all data processing steps**, ensuring that each module runs in sequence. It automates the cleaning and merging of datasets, producing a final structured dataset for analysis.\n",
    "\n",
    "### Steps Executed\n",
    "#### 1. Run Cleaning Scripts\n",
    "- Executes:\n",
    "  - **`census_cleaning.py`** – Prepares census demographic data.\n",
    "  - **`product_cleaning.py`** – Cleans and standardizes grocery product data.\n",
    "  - **`location_cleaning.py`** – Geocodes and formats store location data.\n",
    "\n",
    "#### 2. Merge Data Sources\n",
    "- Calls **`merge_final.py`** to integrate all processed datasets into a single structured dataset.\n",
    "\n",
    "#### 3. Validate Final Dataset\n",
    "- Checks if `final_dataset.csv` was successfully created.\n",
    "- Prints confirmation or error message if merging fails.\n",
    "\n",
    "### Purpose\n",
    "This script ensures that the entire **data pipeline runs smoothly** from raw extraction to a structured dataset, making the data ready for analysis and visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# I felt this was a simpler  approach rather than  constructing a driver class.\n",
    "# If these scripts migrate to an automated pipeline then I will update this accordingly.\n",
    "def run_script(script_name):\n",
    "    \"\"\"Runs a Python script from the src/data_processing/ directory.\"\"\"\n",
    "    try:\n",
    "        subprocess.run([\"python\", script_name], check=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running {script_name}: {e}\")\n",
    "        exit(1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Starting Data Processing Pipeline...\")\n",
    "    \n",
    "    # Step 1: Run Cleaning Scripts\n",
    "    print(\"Cleaning census data...\")\n",
    "    run_script(\"census_cleaning.py\")\n",
    "    \n",
    "    print(\"Cleaning product data...\")\n",
    "    run_script(\"product_cleaning.py\")\n",
    "    \n",
    "    print(\"Cleaning location data...\")\n",
    "    run_script(\"location_cleaning.py\")\n",
    "    \n",
    "    # Step 2: Merge Data\n",
    "    print(\"Merging data sources...\")\n",
    "    run_script(\"merge_final.py\")\n",
    "    \n",
    "    # Step 3: Check if Final Dataset Exists\n",
    "    DATA_FILE = \"../data/final_dataset.csv\"\n",
    "    if os.path.exists(DATA_FILE):\n",
    "        print(f\"Final dataset generated successfully: {DATA_FILE}\")\n",
    "    else:\n",
    "        print(\"Final dataset not found! Check for errors in the merge process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Data Processing Summary\n",
    "\n",
    "### Overview\n",
    "The data processing pipeline extracts, cleans, and integrates multiple datasets to create a structured foundation for analysis. I'm  curious to hear feedback on the way the Product data in particular was managed since averaging averages doesn't feel like the best approach, but I thought it offered a data point that  was representative of ZIP Code overall. \n",
    "\n",
    "### Key Takeaways\n",
    "- **Census Data Cleaning** – Standardized socioeconomic indicators for ZIP-level analysis.\n",
    "- **Location Data Processing** – Geocoded store locations and validated ZIP codes.\n",
    "- **Product Data Standardization** – Categorized products, normalized prices, and removed invalid entries.\n",
    "- **Dataset Merging** – Combined all sources to create a comprehensive dataset for visualization and correlation studies.\n",
    "- **Automated Execution** – A centralized script ensures seamless data processing.\n",
    "\n",
    "### Next Steps\n",
    "With a clean and structured dataset, we can now **explore pricing trends and analyze correlations** across different ZIP codes.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
