{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Acquisition: Fetching Product Data from Kroger API**\n",
    "\n",
    "## **Introduction**\n",
    "This notebook demonstrates the data acquisition pipeline for retrieving product information from the Kroger API. The process involves:\n",
    "\n",
    "- **Authenticating with the Kroger API** using OAuth2.\n",
    "- **Fetching product data** from Kroger locations.\n",
    "- **Filtering and saving the data** for analysis.\n",
    "- **Tracking API calls** to ensure efficient data retrieval.\n",
    "\n",
    "We leverage Python scripts developed in this project:\n",
    "\n",
    "- `main.py`: Orchestrates the data pipeline.\n",
    "- `kroger_api.py`: Handles API authentication and requests.\n",
    "- `fetch_product.py`: Manages data fetching and storage.\n",
    "- `data_processing.py`: Filters and processes product data.\n",
    "- `tracking.py`: Logs API requests and manages location tracking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kroger API Integration**\n",
    "\n",
    "### **Overview**\n",
    "The `kroger_api.py` module handles authentication and data retrieval from the **Kroger Product Compact API**. This script enables access to product details, pricing, and availability for various store locations.\n",
    "\n",
    "### **Key Features**\n",
    "- **OAuth2 Authentication**: Uses client credentials to generate an access token.\n",
    "- **Token Management**: Automatically refreshes the access token when expired.\n",
    "- **Product Search**: Fetches product data using keyword-based search queries.\n",
    "- **Error Handling & Rate Limiting**: Implements retry mechanisms for failed requests and prevents API throttling.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Authenticate** with Kroger API using client ID and secret.\n",
    "2. **Obtain an access token**, storing it in environment variables.\n",
    "3. **Query product data** using location IDs and search terms.\n",
    "4. **Return structured product information** including pricing, category, and stock availability.\n",
    "5. **Handle API errors** such as timeouts, authentication failures, and invalid responses.\n",
    "\n",
    "This module is a core component of the data pipeline, facilitating seamless interaction with Krogerâ€™s API for product data acquisition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import get_key, load_dotenv\n",
    "import requests\n",
    "from requests.exceptions import RequestException\n",
    "import base64\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/acquisition/`\n",
    "DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"data\"))  # Navigate to `src/data/`\n",
    "\n",
    "# Load environment variables\n",
    "ENV_FILE = os.path.join(DATA_DIR,\"kroger_client_info.env\")  # Define .env file path\n",
    "\n",
    "load_dotenv()\n",
    "CLIENT_ID = get_key(ENV_FILE, \"KROGER_CLIENT_ID\")\n",
    "CLIENT_SECRET = get_key(ENV_FILE, \"KROGER_CLIENT_SECRET\")\n",
    "\n",
    "# # Load API credentials from GitHub Actions environment variables\n",
    "# # Preserve in the event actions is restarted\n",
    "# CLIENT_ID = os.getenv(\"KROGER_CLIENT_ID\")\n",
    "# CLIENT_SECRET = os.getenv(\"KROGER_CLIENT_SECRET\")\n",
    "\n",
    "if not CLIENT_ID or not CLIENT_SECRET:\n",
    "    raise ValueError(\"Missing Kroger API credentials in environment variables!\")\n",
    "\n",
    "# Encode credentials for API authentication\n",
    "encoded_auth = base64.b64encode(f\"{CLIENT_ID}:{CLIENT_SECRET}\".encode()).decode()\n",
    "\n",
    "TOKEN_URL = \"https://api-ce.kroger.com/v1/connect/oauth2/token\"\n",
    "HEADERS = {\n",
    "    \"Authorization\": f\"Basic {encoded_auth}\",\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "}\n",
    "\n",
    "# Function to get a new token if expired\n",
    "def get_kroger_product_compact_token():\n",
    "    \"\"\"Retrieve or refresh the Kroger Product Compact API access token.\"\"\"\n",
    "    access_token = os.getenv(\"PRODUCT_COMPACT_ACCESS_TOKEN\")\n",
    "    expiration = os.getenv(\"PRODUCT_COMPACT_ACCESS_TOKEN_EXPIRATION\")\n",
    "\n",
    "    if not access_token or datetime.now() >= datetime.fromisoformat(expiration):\n",
    "        print(\"ðŸ”„ Token expired or missing, requesting a new one...\")\n",
    "\n",
    "        response = requests.post(TOKEN_URL, headers=HEADERS, data=\"grant_type=client_credentials&scope=product.compact\")\n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            access_token = response_data.get(\"access_token\")\n",
    "            expires_in = response_data.get(\"expires_in\", 1800)\n",
    "            expiration_time = datetime.now() + timedelta(seconds=expires_in)\n",
    "\n",
    "            # Save new tokens in environment for next steps\n",
    "            os.environ[\"PRODUCT_COMPACT_ACCESS_TOKEN\"] = access_token\n",
    "            os.environ[\"PRODUCT_COMPACT_ACCESS_TOKEN_EXPIRATION\"] = expiration_time.isoformat()\n",
    "\n",
    "            print(\"New Token Retrieved!\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Failed to retrieve token: {response.json()}\")\n",
    "\n",
    "    return access_token\n",
    "\n",
    "# Ensure API base URL is set\n",
    "PRODUCTS_API_URL = \"https://api-ce.kroger.com/v1/products\"\n",
    "\n",
    "def search_kroger_products(location_id, search_terms=[\"Eggs\", \"Bread\"], limit=50):\n",
    "    \"\"\"Search for products at a Kroger location with pagination and enhanced error handling.\"\"\"\n",
    "    \n",
    "    token = get_kroger_product_compact_token()\n",
    "    if not token:\n",
    "        print(\"Failed to retrieve API token. Skipping product search.\")\n",
    "        return None\n",
    "\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\", \"Accept\": \"application/json\"}\n",
    "    all_products = []\n",
    "    max_pages = 5\n",
    "\n",
    "    for term in search_terms:\n",
    "        start = 1\n",
    "        for page in range(max_pages):\n",
    "            params = {\n",
    "                \"filter.term\": term,\n",
    "                \"filter.locationId\": location_id,\n",
    "                \"filter.limit\": limit,\n",
    "                \"filter.start\": start,\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                response = requests.get(PRODUCTS_API_URL, headers=headers, params=params)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    data = response.json()\n",
    "                    \n",
    "                    if \"data\" in data:\n",
    "                        for product in data[\"data\"]:\n",
    "                            product[\"locationId\"] = location_id  # Ensure location ID is stored\n",
    "                        all_products.extend(data[\"data\"])\n",
    "\n",
    "                        # Check pagination limits\n",
    "                        total_results = data.get(\"meta\", {}).get(\"pagination\", {}).get(\"total\", 0)\n",
    "                        if len(all_products) >= total_results or len(data[\"data\"]) < limit:\n",
    "                            break  # Stop pagination if we've fetched all results\n",
    "\n",
    "                        # Move to next batch\n",
    "                        start += limit\n",
    "                        time.sleep(1)  # Prevent rate-limiting\n",
    "                    else:\n",
    "                        print(f\"No more products found for '{term}' at location {location_id}.\")\n",
    "                        break\n",
    "                else:\n",
    "                    print(f\"API Error ({response.status_code}) fetching '{term}': {response.json()}\")\n",
    "                    break\n",
    "\n",
    "            except RequestException as e:\n",
    "                print(f\"Network error while fetching '{term}' at location {location_id}: {e}\")\n",
    "                break  # Exit loop on request failure\n",
    "\n",
    "    return all_products\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Product Data Retrieval**\n",
    "\n",
    "### **Overview**\n",
    "The `fetch_product.py` module is responsible for **fetching, filtering, and storing** product data from the Kroger API. It acts as the intermediary between the API and the local dataset, ensuring only relevant products are retained.\n",
    "\n",
    "### **Key Features**\n",
    "- **Batch Processing**: Iterates through multiple store locations, retrieving product data efficiently.\n",
    "- **Product Filtering**: Applies category and keyword-based filtering to retain only relevant products.\n",
    "- **Data Storage**: Saves retrieved and filtered data into structured CSV files.\n",
    "- **Error Handling & Logging**: Captures API failures, logs progress, and prevents redundant API calls.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Fetch Products**: Calls the `search_kroger_products()` function from `kroger_api.py` for a given store location.\n",
    "2. **Extract Relevant Data**: Retrieves product details, including brand, description, price, and stock levels.\n",
    "3. **Apply Filtering**: Uses predefined criteria (categories and keywords) to retain essential products.\n",
    "4. **Save Data**: Appends the filtered data to `kroger_product_data.csv`.\n",
    "5. **Update Tracker**: Marks locations as successfully queried to prevent unnecessary repeat calls.\n",
    "\n",
    "### **Dependencies**\n",
    "- `kroger_api.py` for making API requests.\n",
    "- `data_processing.py` for product filtering.\n",
    "- `tracking.py` for updating the API call log.\n",
    "\n",
    "This module ensures that the data pipeline retrieves and processes only **relevant grocery product data** while efficiently managing API resources.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from acquisition.kroger_api import search_kroger_products\n",
    "from acquisition.data_processing import filter_products, save_to_csv\n",
    "from acquisition.tracking import update_tracker, update_log\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/acquisition/`\n",
    "DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"data\"))  # Navigate to `src/data/`\n",
    "\n",
    "# # Set base directory dynamically\n",
    "# # Preserve for reference\n",
    "# BASE_DIR = os.getenv(\"GITHUB_WORKSPACE\", os.path.dirname(os.path.abspath(__file__)))\n",
    "# DATA_DIR = os.path.join(BASE_DIR, \"kroger-data-pipeline\", \"src\", \"data\")\n",
    "\n",
    "# Update file paths\n",
    "PRODUCTS_FILE = os.path.join(DATA_DIR, \"kroger_product_data.csv\")\n",
    "LOCATION_FILE = os.path.join(DATA_DIR, \"kroger_locations.csv\")\n",
    "PRODUCT_API_LOG = os.path.join(DATA_DIR, \"product_api_log.csv\")\n",
    "\n",
    "# Ensure data folder exists in GitHub Actions runner\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"Created missing data directory: {DATA_DIR}\")\n",
    "\n",
    "def fetch_and_filter_products(location_id):\n",
    "    \"\"\"Fetch products from Kroger API, filter relevant ones, and save to CSV.\"\"\"\n",
    "    \n",
    "    # Call API (retrieves all products from Dairy & Bakery)\n",
    "    product_results = search_kroger_products(location_id=location_id)\n",
    "    \n",
    "    if not product_results:\n",
    "        print(f\"âš ï¸ No results found for Location ID {location_id}\")\n",
    "        return None\n",
    "\n",
    "    filtered_products = []\n",
    "\n",
    "    if product_results:  # Ensure data exists\n",
    "        for product in product_results:\n",
    "            # Extract core product details\n",
    "            product_id = product.get(\"productId\", \"Unknown\")\n",
    "            upc = product.get(\"upc\", \"Unknown\")\n",
    "            brand = product.get(\"brand\", \"Unknown\")\n",
    "            description = product.get(\"description\", \"Unknown\")\n",
    "            category = \", \".join(product.get(\"categories\", []))  # Convert list to string\n",
    "            location_id = product.get(\"locationId\", \"UNKNOWN_LOCATION\")  # Ensure location ID is included\n",
    "            \n",
    "            # Extract pricing & availability\n",
    "            price_info = product.get(\"items\", [{}])[0]  # Get first item\n",
    "            regular_price = price_info.get(\"price\", {}).get(\"regular\", 0)\n",
    "            promo_price = price_info.get(\"price\", {}).get(\"promo\", 0)\n",
    "            stock_level = price_info.get(\"inventory\", {}).get(\"stockLevel\", \"Unknown\")\n",
    "            \n",
    "            # Extract packaging details\n",
    "            size = price_info.get(\"size\", \"Unknown\")\n",
    "            sold_by = price_info.get(\"soldBy\", \"Unknown\")\n",
    "            \n",
    "            # Add date retrieved\n",
    "            date_retrieved = datetime.date.today().strftime('%Y-%m-%d')\n",
    "            \n",
    "            # Store filtered product data\n",
    "            filtered_products.append({\n",
    "                \"Product ID\": product_id,\n",
    "                \"UPC\": upc,\n",
    "                \"Brand\": brand,\n",
    "                \"Description\": description,\n",
    "                \"Category\": category,\n",
    "                \"Location ID\": location_id,\n",
    "                \"Regular Price\": regular_price,\n",
    "                \"Promo Price\": promo_price,\n",
    "                \"Stock Level\": stock_level,\n",
    "                \"Size\": size,\n",
    "                \"Sold By\": sold_by,\n",
    "                \"Date Retrieved\": date_retrieved\n",
    "            })\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    products_df = pd.DataFrame(filtered_products)\n",
    "    \n",
    "    # Explicitly set correct data types before saving\n",
    "    products_df = products_df.astype({\n",
    "        \"Product ID\": str,\n",
    "        \"UPC\": str,\n",
    "        \"Location ID\": str\n",
    "    }, errors=\"ignore\")  # Prevent errors if some values can't be converted\n",
    "\n",
    "    # Apply Filtering Step (Using `filter_products`)\n",
    "    filtered_df = filter_products(products_df)\n",
    "\n",
    "    # Save to CSV with correct pathing\n",
    "    save_to_csv(filtered_df, PRODUCTS_FILE)\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "def fetch_products_in_batches(batch_size=10):\n",
    "    \"\"\"Iterates over locations that need data and fetches products.\"\"\"    \n",
    "    # Refresh needs_data status before fetching\n",
    "    update_log()\n",
    "    \n",
    "    tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "    \n",
    "    # Select only locations that need data\n",
    "    locations_to_process = tracker_df[tracker_df[\"Needs Data\"] == True][\"Location ID\"].tolist()\n",
    "    \n",
    "    total_locations = len(locations_to_process)\n",
    "    print(f\"{total_locations} require updates, this run will process {batch_size} when complete.\")\n",
    "    processed_count = 0\n",
    "    processed_locations = []\n",
    "\n",
    "    for i, location_id in enumerate(locations_to_process, start=1):\n",
    "        try:\n",
    "            fetch_and_filter_products(location_id)  # Calls the API and saves data\n",
    "            processed_locations.append(location_id)\n",
    "            processed_count += 1\n",
    "\n",
    "            # Mental sanity feature so I don't constantly wonder how many records have been processed    \n",
    "            sys.stdout.write(f\"\\rðŸ”„ Progress: {i}/{batch_size} ZIP codes processed\")\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            # Stop after reaching batch limit\n",
    "            if processed_count >= batch_size:\n",
    "                break\n",
    "\n",
    "            # Prevent rate limiting\n",
    "            if processed_count < batch_size:\n",
    "                time.sleep(2)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error processing {location_id}: {str(e)}\")\n",
    "\n",
    "    print(f\"\\nCompleted {processed_count} API calls in this run.\")\n",
    "    \n",
    "    return processed_locations\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    fetch_products_in_batches(batch_size=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Processing & Filtering**\n",
    "\n",
    "### **Overview**\n",
    "The `data_processing.py` module is responsible for **structuring, filtering, and managing** the product data retrieved from the Kroger API. It ensures that only relevant data is retained and optimally stored for further analysis.\n",
    "\n",
    "### **Key Features**\n",
    "- **File Management**: Ensures data directories and CSV files exist before writing data.\n",
    "- **Product Filtering**: Selects products based on predefined **categories** and **keywords** (e.g., \"Dairy\", \"Bakery\").\n",
    "- **Tracker Initialization**: Maintains and updates API request logs to monitor data freshness.\n",
    "- **Data Cleaning**: Removes duplicate records and updates tracking logs for stale data.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Initialize Files**: Ensures that required CSV files exist (`kroger_product_data.csv`, `product_api_log.csv`).\n",
    "2. **Filter Products**: Uses category and keyword-based filtering to retain only necessary items.\n",
    "3. **Update API Tracker**: Refreshes `product_api_log.csv` to flag locations requiring updates.\n",
    "4. **Remove Duplicates**: Cleans up redundant data entries in the tracking log.\n",
    "5. **Save Processed Data**: Stores filtered results in `kroger_product_data.csv` for downstream analysis.\n",
    "\n",
    "### **Dependencies**\n",
    "- `tracking.py` for managing API call logs.\n",
    "\n",
    "This module ensures **efficient data storage and retrieval**, keeping the dataset clean, relevant, and ready for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/acquisition/`\n",
    "DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"data\"))  # Navigate to `src/data/`\n",
    "\n",
    "# # Set base directory dynamically\n",
    "# BASE_DIR = os.getenv(\"GITHUB_WORKSPACE\", os.path.dirname(os.path.abspath(__file__)))\n",
    "# DATA_DIR = os.path.join(BASE_DIR, \"kroger-data-pipeline\", \"src\", \"data\")\n",
    "\n",
    "# Update file paths\n",
    "PRODUCTS_FILE = os.path.join(DATA_DIR, \"kroger_product_data.csv\")\n",
    "LOCATION_FILE = os.path.join(DATA_DIR, \"kroger_locations.csv\")\n",
    "PRODUCT_API_LOG = os.path.join(DATA_DIR, \"product_api_log.csv\")\n",
    "\n",
    "# Ensure data folder exists in GitHub Actions runner\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"Created missing data directory: {DATA_DIR}\")\n",
    "\n",
    "# Ensure the product data CSV exists (Creates empty file if missing)\n",
    "if not os.path.exists(PRODUCTS_FILE):\n",
    "    pd.DataFrame(columns=[\n",
    "        \"Product ID\", \"UPC\", \"Brand\", \"Description\", \"Category\", \"Location ID\", \n",
    "        \"Regular Price\", \"Promo Price\", \"Stock Level\", \"Size\", \"Sold By\", \"Date Retrieved\"\n",
    "    ]).to_csv(PRODUCTS_FILE, index=False)\n",
    "    print(f\"Created new CSV file: {PRODUCTS_FILE}\")\n",
    "\n",
    "\n",
    "def initialize_tracker():\n",
    "    \"\"\"Initializes or loads the location tracking file.\"\"\"\n",
    "    locations_df = pd.read_csv(LOCATION_FILE, dtype={\"Location ID\": str})\n",
    "\n",
    "    if os.path.exists(PRODUCT_API_LOG):\n",
    "        print(\"ðŸ”„ Loading existing product API tracker...\")\n",
    "        tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "    else:\n",
    "        print(\"ðŸ†• Creating new product tracker file...\")\n",
    "        tracker_df = pd.DataFrame({\n",
    "            \"Location ID\": locations_df[\"Location ID\"].astype(str),\n",
    "            \"Last Retrieved Date\": \"\",\n",
    "            \"Successful Calls\": 0,\n",
    "            \"Needs Data\": True\n",
    "        })\n",
    "        tracker_df.to_csv(PRODUCT_API_LOG, index=False)\n",
    "        print(f\"Created new tracker file: {PRODUCT_API_LOG}\")\n",
    "\n",
    "    return tracker_df\n",
    "\n",
    "\n",
    "def update_tracker(location_id):\n",
    "    \"\"\"Updates the tracker after a successful API call.\"\"\"\n",
    "    tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "\n",
    "    # Update Last Retrieved Date & Increment Successful Calls\n",
    "    today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    tracker_df.loc[tracker_df[\"Location ID\"] == location_id, \"Last Retrieved Date\"] = today_str\n",
    "    tracker_df.loc[tracker_df[\"Location ID\"] == location_id, \"Successful Calls\"] += 1\n",
    "\n",
    "    tracker_df.to_csv(PRODUCT_API_LOG, index=False)\n",
    "    print(f\"Tracker updated for Location ID {location_id}\")\n",
    "\n",
    "\n",
    "def update_log():\n",
    "    \"\"\"Updates 'Needs Data' column based on last retrieval date and removes duplicates.\"\"\"\n",
    "    tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "\n",
    "    # **Remove Duplicates**\n",
    "    tracker_df = tracker_df.drop_duplicates(subset=[\"Location ID\"], keep=\"last\")\n",
    "\n",
    "    # **Optimize Date Processing (Vectorized Approach)**\n",
    "    tracker_df[\"Last Retrieved Date\"] = pd.to_datetime(tracker_df[\"Last Retrieved Date\"], errors=\"coerce\")\n",
    "\n",
    "    # **Set 'Needs Data' True if never retrieved OR 7+ days since last call**\n",
    "    tracker_df[\"Needs Data\"] = tracker_df[\"Last Retrieved Date\"].isna() | \\\n",
    "                               (datetime.today() - tracker_df[\"Last Retrieved Date\"]).dt.days >= 7\n",
    "\n",
    "    # Save the cleaned tracker\n",
    "    tracker_df.to_csv(PRODUCT_API_LOG, index=False)\n",
    "    print(\"'Needs Data' column refreshed and duplicates removed.\")\n",
    "\n",
    "\n",
    "def filter_products(df):\n",
    "    \"\"\"Filter products based on relevant categories and keywords.\"\"\"\n",
    "    \n",
    "    valid_categories = [\"Dairy\", \"Bakery\"]\n",
    "    valid_keywords = [\"egg\", \"bread\"]\n",
    "\n",
    "    return df[\n",
    "        df[\"Category\"].apply(lambda x: any(cat in x for cat in valid_categories)) &\n",
    "        df[\"Description\"].str.lower().apply(lambda x: any(kw in x for kw in valid_keywords))\n",
    "    ]\n",
    "\n",
    "\n",
    "def save_to_csv(df, filename):\n",
    "    \"\"\"Append DataFrame to CSV file or create new file if missing.\"\"\"\n",
    "    \n",
    "    if df.empty:\n",
    "        print(\"âš ï¸ No relevant products found, skipping file update.\")\n",
    "        return\n",
    "    \n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "\n",
    "    if os.path.exists(filename):\n",
    "        existing_df = pd.read_csv(filename)\n",
    "        df = pd.concat([existing_df, df], ignore_index=True)\n",
    "\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Filtered data saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **API Call Tracking & Management**\n",
    "\n",
    "### **Overview**\n",
    "The `tracking.py` module is responsible for **logging, monitoring, and managing** API requests to the Kroger API. It ensures that data retrieval is efficient by keeping track of which store locations have been queried and when they need updates.\n",
    "\n",
    "### **Key Features**\n",
    "- **API Call Logging**: Maintains a record of successful API calls for each store location.\n",
    "- **Data Freshness Tracking**: Flags locations needing updates based on a **7-day** refresh cycle.\n",
    "- **Duplicate Removal**: Cleans up redundant entries in tracking logs.\n",
    "- **Automatic File Initialization**: Ensures tracking files exist before execution.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Initialize Tracking File**: If `product_api_log.csv` is missing, it creates a new file with default values.\n",
    "2. **Load Existing Logs**: Reads the current API call history from `product_api_log.csv`.\n",
    "3. **Update Tracker**: Marks successful API calls and timestamps the last retrieval date.\n",
    "4. **Check Data Freshness**: Flags locations as needing updates if the last retrieval was **7+ days ago**.\n",
    "5. **Save Updates**: Writes the refreshed tracker back to `product_api_log.csv`.\n",
    "\n",
    "This module ensures that API requests are **efficiently managed** and prevents redundant data retrieval while maintaining data freshness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/acquisition/`\n",
    "DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"data\"))  # Navigate to `src/data/`\n",
    "\n",
    "# # Set base directory dynamically\n",
    "# BASE_DIR = os.getenv(\"GITHUB_WORKSPACE\", os.path.dirname(os.path.abspath(__file__)))\n",
    "# DATA_DIR = os.path.join(BASE_DIR, \"kroger-data-pipeline\", \"src\", \"data\")\n",
    "\n",
    "# Update file paths\n",
    "PRODUCTS_FILE = os.path.join(DATA_DIR, \"kroger_product_data.csv\")\n",
    "LOCATION_FILE = os.path.join(DATA_DIR, \"kroger_locations.csv\")\n",
    "PRODUCT_API_LOG = os.path.join(DATA_DIR, \"product_api_log.csv\")\n",
    "\n",
    "\n",
    "# Ensure data folder exists in GitHub Actions runner\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    print(f\"Created missing data directory: {DATA_DIR}\")\n",
    "\n",
    "print(f\"ðŸ“‚ Looking for location file in: {LOCATION_FILE}\")\n",
    "print(f\"ðŸ“‚ Looking for product log in: {PRODUCT_API_LOG}\")\n",
    "\n",
    "# Ensure 'data' folder exists\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise FileNotFoundError(f\"âŒ Data directory is missing: {DATA_DIR}\")\n",
    "\n",
    "# Ensure 'kroger_locations.csv' exists\n",
    "if not os.path.exists(LOCATION_FILE):\n",
    "    raise FileNotFoundError(f\"âŒ Missing location file: {LOCATION_FILE}\")\n",
    "\n",
    "# Ensure 'PRODUCT_API_LOG.csv' exists\n",
    "if not os.path.exists(PRODUCT_API_LOG):\n",
    "    print(\"âš ï¸ Tracker file missing! Creating new file...\")\n",
    "    pd.DataFrame(columns=[\"Location ID\", \"Last Retrieved Date\", \"Successful Calls\", \"Needs Data\"]).to_csv(PRODUCT_API_LOG, index=False)\n",
    "\n",
    "def load_location_tracker():\n",
    "    \"\"\"Loads the location tracker and initializes missing locations.\"\"\"\n",
    "    if not os.path.exists(LOCATION_FILE):\n",
    "        raise FileNotFoundError(f\"âŒ Missing location file: {LOCATION_FILE}\")\n",
    "\n",
    "    locations_df = pd.read_csv(LOCATION_FILE, dtype={\"Location ID\": str})\n",
    "    location_ids = locations_df[\"Location ID\"].astype(str).tolist()\n",
    "\n",
    "    if os.path.exists(PRODUCT_API_LOG):\n",
    "        print(\"ðŸ”„ Loading existing product API tracker...\")\n",
    "        tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "        \n",
    "        # Ensure column types are correct\n",
    "        tracker_df[\"Last Retrieved Date\"] = tracker_df[\"Last Retrieved Date\"].astype(str)\n",
    "        tracker_df[\"Successful Calls\"] = tracker_df[\"Successful Calls\"].astype(int)\n",
    "        tracker_df[\"Needs Data\"] = tracker_df[\"Needs Data\"].astype(bool)\n",
    "    else:\n",
    "        print(\"ðŸ†• Creating new product tracker file...\")\n",
    "        tracker_df = pd.DataFrame({\n",
    "            \"Location ID\": location_ids,\n",
    "            \"Last Retrieved Date\": \"\",\n",
    "            \"Successful Calls\": 0,\n",
    "            \"Needs Data\": True\n",
    "        })\n",
    "        tracker_df.to_csv(PRODUCT_API_LOG, index=False)\n",
    "\n",
    "    return tracker_df\n",
    "\n",
    "def update_tracker(location_id):\n",
    "    \"\"\"Updates the tracker after a successful API call.\"\"\"\n",
    "    tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "\n",
    "    today_str = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    # Ensure we're updating the correct row\n",
    "    tracker_df.loc[tracker_df[\"Location ID\"] == location_id, \"Last Retrieved Date\"] = today_str\n",
    "    tracker_df.loc[tracker_df[\"Location ID\"] == location_id, \"Successful Calls\"] += 1\n",
    "\n",
    "    print(f\"ðŸ“ Updating tracker: {location_id} - {today_str}\")\n",
    "\n",
    "    tracker_df.to_csv(PRODUCT_API_LOG, index=False)  # Ensure changes are saved\n",
    "\n",
    "def update_log():\n",
    "    \"\"\"Updates 'Needs Data' column based on the last retrieval date and removes duplicates.\"\"\"\n",
    "    tracker_df = pd.read_csv(PRODUCT_API_LOG, dtype={\"Location ID\": str})\n",
    "\n",
    "    # Remove Duplicate Entries Based on 'Location ID'\n",
    "    tracker_df = tracker_df.drop_duplicates(subset=[\"Location ID\"], keep=\"last\")\n",
    "\n",
    "    def check_needs_data(row):\n",
    "        last_call = str(row[\"Last Retrieved Date\"])  # Convert NaN to string\n",
    "\n",
    "        # Handle empty or invalid dates gracefully\n",
    "        if last_call in [\"\", \"nan\", \"NaT\", \"None\"] or pd.isna(row[\"Last Retrieved Date\"]):\n",
    "            return True  # No previous retrieval, needs data\n",
    "\n",
    "        try:\n",
    "            last_call_date = datetime.strptime(last_call, \"%Y-%m-%d\")\n",
    "            return (datetime.today() - last_call_date).days >= 7\n",
    "        except ValueError:\n",
    "            return True  # Handle invalid date format\n",
    "\n",
    "    # Apply function to update 'Needs Data' column\n",
    "    tracker_df[\"Needs Data\"] = tracker_df.apply(check_needs_data, axis=1)\n",
    "\n",
    "    # Save the updated tracker, ensuring no duplicates\n",
    "    tracker_df.to_csv(PRODUCT_API_LOG, index=False)\n",
    "    print(\"'Needs Data' column refreshed and duplicates removed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Kroger Data Pipeline Orchestration**\n",
    "\n",
    "### **Overview**\n",
    "The `main.py` script serves as the **central orchestrator** of the Kroger data pipeline. It coordinates the execution of various modules to ensure **efficient data acquisition, processing, and tracking**.\n",
    "\n",
    "### **Key Features**\n",
    "- **Pipeline Execution**: Automates the end-to-end process of fetching and processing product data.\n",
    "- **API Token Management**: Ensures a valid authentication token is available before querying the API.\n",
    "- **Batch Processing**: Fetches data in controlled batches to optimize API usage.\n",
    "- **Tracking & Logging**: Updates logs to prevent redundant API calls.\n",
    "- **Data Filtering & Storage**: Cleans and stores retrieved product data in structured files.\n",
    "\n",
    "### **How It Works**\n",
    "1. **Authenticate**: Retrieves a valid Kroger API access token.\n",
    "2. **Initialize Tracker**: Loads and updates API call logs to determine which locations need data.\n",
    "3. **Fetch Product Data**: Calls `fetch_product.py` to retrieve and filter grocery product details.\n",
    "4. **Update API Logs**: Marks successful API calls to prevent redundant queries.\n",
    "5. **Filter & Save Data**: Applies `data_processing.py` logic to retain only relevant products.\n",
    "6. **Complete Execution**: Saves all updates and logs completion.\n",
    "\n",
    "### **Dependencies**\n",
    "- `fetch_product.py` for retrieving and filtering data.\n",
    "- `kroger_api.py` for API authentication and product queries.\n",
    "- `data_processing.py` for cleaning and storing data.\n",
    "- `tracking.py` for managing API request logs.\n",
    "\n",
    "This script automates the **entire data pipeline**, ensuring that product data retrieval from Kroger is **structured, optimized, and repeatable**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "\n",
    "# Set up directory paths\n",
    "BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/acquisition/`\n",
    "DATA_DIR = os.path.abspath(os.path.join(BASE_DIR, \"..\", \"data\"))  # Navigate to `src/data/`\n",
    "PROJECT_ROOT = os.path.dirname(BASE_DIR)\n",
    "sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "from acquisition.tracking import load_location_tracker, update_log, update_tracker\n",
    "from acquisition.fetch_product import fetch_products_in_batches\n",
    "from acquisition.kroger_api import get_kroger_product_compact_token\n",
    "from acquisition.data_processing import filter_products, save_to_csv\n",
    "\n",
    "# # Set up directory paths\n",
    "# BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # Get `src/` directory\n",
    "# DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
    "\n",
    "# Define file paths\n",
    "PRODUCTS_FILE = os.path.join(DATA_DIR, \"kroger_product_data.csv\")\n",
    "PRODUCT_API_LOG = os.path.join(DATA_DIR, \"product_api_log.csv\")\n",
    "\n",
    "print(f\"PRODUCTS_FILE: {PRODUCTS_FILE}\")\n",
    "print(f\"PRODUCT_API_LOG: {PRODUCT_API_LOG}\")\n",
    "\n",
    "def main(batch_size=10):\n",
    "    \"\"\"Orchestrates the full Kroger data pipeline.\"\"\"\n",
    "    \n",
    "    print(\"\\n**Starting Kroger Data Pipeline**\\n\")\n",
    "\n",
    "    # Step 1: Ensure API Token is Valid\n",
    "    token = get_kroger_product_compact_token()\n",
    "    if not token:\n",
    "        print(\"Failed to retrieve API token. Exiting...\")\n",
    "        return\n",
    "    \n",
    "    # Step 2: Initialize & Update Logs\n",
    "    print(\"Loading tracker & checking logs...\")\n",
    "    load_location_tracker()\n",
    "    update_log()  # Mark locations that need updates\n",
    "\n",
    "    # Step 3: Fetch & Process Product Data\n",
    "    print(\"Fetching product data...\")\n",
    "    updated_locations = fetch_products_in_batches(batch_size=batch_size)\n",
    "\n",
    "    # Step 4: Apply Updates After Fetching\n",
    "    print(\"Updating tracker after batch fetch...\")\n",
    "    for location_id in updated_locations:\n",
    "        update_tracker(location_id)  # Ensure updates are applied after processing\n",
    "\n",
    "    # Step 5: Load & Filter Data\n",
    "    print(\"Processing and filtering product data...\")\n",
    "    if os.path.exists(PRODUCTS_FILE):\n",
    "        df = pd.read_csv(PRODUCTS_FILE)\n",
    "        filtered_df = filter_products(df)\n",
    "        save_to_csv(filtered_df, PRODUCTS_FILE)\n",
    "        print(f\"Processed and saved filtered product data to {PRODUCTS_FILE}\")\n",
    "    else:\n",
    "        print(\"No product data file found! Skipping filtering step.\")\n",
    "    \n",
    "    print(\"\\n**Pipeline Execution Complete!**\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(batch_size=1)  # Adjust batch size as needed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
